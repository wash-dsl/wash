\chapter{Resources}

Throughout the development of the project, we anticipate requiring several open source technologies and projects. These will vastly aid development, by saving time re-implementing common functions such as hardware compatibility or control, or by providing comparisons of performance and results for testing efficiency or validating program correctness. Furthermore, high-end multi-node multi-core computing systems are prohibitively expensive, so for practical purposes it is more feasible to utilise these systems. 

% Stuff which will be used to test the DSL against
\section{Miniapps}

Miniapp (or ``Mini-App'') is a miniature application which captures the inherent computational complexity of the problem domain whilst still keeping a small enough codebase to be manually analysed (or optimised). The term was coined fairly recently, and while there is no concrete definition of what a miniapp \textit{is}, they also resemble closely benchmark applications developed previously \cite{miniapp}.

We are planning on using two miniapps (SPH-EXA \cite{sphexa} and PBF-SPH \cite{pbfsph}), which both implement SPH simulations for a number of different test scenarios, each targeting a number of different backends. 

The use of miniapps in this project will be twofold: Firstly, as a benchmark. The miniapps are hand optimised to run on specific backend technologies such as PBH-SPH which runs on OpenMP, MPI, CUDA, OpenCL and SYCL. By comparing the run time of programs automatically generated by the project to these hand optimised programs we can get a benchmark of how much overhead our project adds to the expected performance a user would expect.

Secondly, we can use the miniapps to provide a simple verification check that the project is outputting the expected solutions to the simulation. The SPH-EXA miniapp includes a number of test simulations such as ``sedov'' \cite{sedov} (spherical blast wave), ``noh'' (spherical implosion), ``evrard'' (gravitational collapse of an isothermal cloud), ``turbulence'' (turbulence in a box) and ``kelvin-helmholtz'' (Kelvin-Helmholtz instability in a slice). This will be in addition to other measures taken to ensure program verification and validation detailed in the testing chapter.

% OpenMP, MPI, CUDA, OpenCL, etc?
\section{Libraries}

As previously mentioned, the intention of this project is to develop a portable language which supports a number of high performance computing backends. These are often specialised or dedicated hardware architectures which require custom compiler wrappers or even open source libraries to provide compatibility to a higher level C++ API. 

The backends we aspire to be able to support are the following. Note that due to time constraints, we may have to prioritise the initial and more common backends.  
\begin{itemize}
    \item \textbf{Serial} - Technically this is the absence of a specialised backend. However, it is important to note that the programs should be capable of being run in a serial environment. Perhaps taking advantage of other CPU optimisations such as SIMD vectorisation (i.e. AVX) though Intel Intrinsics library \cite{intrinsics}.
    \item \textbf{OpenMP} - This is the main library which supports shared memory parallel programing on CPUs in C++. It is intended that this be a primary focus for the project \cite{openmp}.
    \item \textbf{MPI} - Message Passing Interface is a standardised paradigm for multi-node parallel computing architectures. This involves orchestrating multiple independent CPUs through message passing. MPI is commonly implemented through an open source library, and we are planning on using OpenMPI \cite{openmpi}.
    \item \textbf{CUDA} - CUDA is the primary parallel computing model developed by Nvidia for GPUs. CUDA is available in C++ through the Nvidia provided library and nvcc compiler \cite{nvcc}.
    \item \textbf{SYCL \& OpenCL} - A group of related technologies which are an open source alternative for developing parallel compute programs on GPUs \cite{sycl}. 
    
    % TODO: more on this?
    % TODO: any more backends?
\end{itemize}

In addition to the above general purpose libraries, the project is planned to involve a code generation functionality to create the optimised code for each backend. One method to implement this from a high level standpoint is to use Jinja \cite{jinja} templates. These are short code snippets defined in a high level templating language called Jinja which are then placed in source code files before being compiled. Another option is to instead use the LLVM \cite{llvm} parser and then directly manipulate the generated abstract syntax tree of the program. 

Furthermore, the nature of particle simulations is that there will be a large amount of data to be dumped out from running simulations for visualisation or debugging purposes. One way of managing this in a parallel computing friendly way, as well as keeping file sizes small, is using a format such as HDF5 \cite{hdf5}, which provides a structured and compact storage medium for the data. In addition, the Silo \cite{silo} library from LLNL\footnote{Lawrence Livermore National Laboratory} allows the stored data to be easily read by other programs, such as Python for analysis, or open source visualisation platforms like VisIt \cite{visit}.

% DCS Batch compute? etc?
\section{Hardware}

To take advantage of the full potential of the massively parallel architectures we plan to use we will require the use of a much more powerful computing system than is available in personal computing hardware. We therefore plan to make use of two systems available at the University of Warwick:

\begin{itemize}
    \item \textbf{DCS Batch Compute} - The Department of Computer Science's batch compute system allows for both CPU and GPU tests involving both single node and multi-node programs. The CPU partitions utilise Intel Xeon E5-2660 v3 with 5 nodes / 200 threads available, while the GPU partitions utilise Nvidia A10s. \\ 
    This system is also particularly useful for consistent benchmarking as they have fewer background processes running on them compared to traditional machines.
    \item \textbf{Scientific Computing RTP} - The University also provides access to more powerful compute clusters which are part of HPC Midlands \cite{scrtp}.
\end{itemize}

% GitHub, overleaf and stuff the project will need to aid development
\section{Software}

In addition to the specialised hardware and libraries used to develop the project, we plan to use other software services to support project development.
\begin{itemize}
    \item \textbf{GitHub} - The university provides an enterprise GitHub solution, which will be useful for version control and managing contributions between a large team working on multiple parts of the project simultaneously. It will also support the project management portion with GitHub Projects, providing a simple Kanban board implementation to keep track of important tasks and pending deadlines. Furthermore, GitHub Actions will allow automated testing to be run regularly to ensure correctness. 
    
    \item \textbf{Visual Studio Code} - A lightweight IDE which supports C++ development as well as git version control. In addition, the remote functionality will enable development on target architecture as well. 
   
    \item \textbf{Slurm} - A job scheduler used by the department to schedule jobs on the specific hardware. It will also allow deploying tasks and monitoring their status automatically via email, instead of having to stay at terminals continually during runtime. 

    \item \textbf{GTest} \cite{gtest} - A C++ unit testing framework developed by Google, which allows functions to be tested and unit tests to be automated through a CI tool such as GitHub Actions.

    % TODO: more?
\end{itemize}